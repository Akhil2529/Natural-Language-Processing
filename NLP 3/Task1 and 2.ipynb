{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdzFZCGKxKLE"
      },
      "source": [
        "## Q1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Rdvb4-uExKLH"
      },
      "outputs": [],
      "source": [
        "number = 1733"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmTry74IxKLI",
        "outputId": "d4ace882-b514-4b64-d19b-54c82a59cd16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import Word2Vec\n",
        "import re\n",
        "import os\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove special characters and punctuation\n",
        "    text = re.sub('[^A-Za-z0-9\\s]+', '', text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove stopwords\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "    return text\n",
        "\n",
        "def train_word_embeddings(data, embedding_size=100, window_size=5, min_count=1):\n",
        "    # Preprocess text\n",
        "    preprocessed_data = [preprocess_text(text) for text in data]\n",
        "    # Convert preprocessed text to list of sentences\n",
        "    sentences = [text.split() for text in preprocessed_data]\n",
        "    # Train word embeddings\n",
        "    #sentences, vector_size=embedding_size, window=window_size, min_count=min_count, workers=4, epochs=10\n",
        "    model = Word2Vec(sentences, vector_size=embedding_size, window=window_size, min_count=min_count, workers=4, epochs=10)\n",
        "    return model\n",
        "\n",
        "# Load dataset\n",
        "data = []\n",
        "labels = []\n",
        "for label in ['neg', 'pos']:\n",
        "    folder_path = os.path.join(r'/content/drive/MyDrive/Comments', label)\n",
        "    for file in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, file)\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "            data.append(preprocess_text(text))\n",
        "            labels.append(label)\n",
        "le = LabelEncoder()\n",
        "labels = le.fit_transform(labels)\n",
        "# Split dataset into training, validation and testing subsets\n",
        "train_size = int(0.7 * len(data))\n",
        "val_size = int(0.15 * len(data))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=number)\n",
        "\n",
        "# train_data = data[:train_size]\n",
        "# train_labels = labels[:train_size]\n",
        "# val_data = data[train_size:train_size+val_size]\n",
        "# val_labels = labels[train_size:train_size+val_size]\n",
        "# test_data = data[train_size+val_size:]\n",
        "# test_labels = labels[train_size+val_size:]\n",
        "\n",
        "# Train word embeddings\n",
        "embedding_size = 100\n",
        "window_size = 3\n",
        "min_count = 1\n",
        "\n",
        "word_embeddings = train_word_embeddings(X_train, embedding_size=embedding_size, window_size=window_size, min_count=min_count)\n"
      ],
      "metadata": {
        "id": "-cZem0YM8CqJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=number)"
      ],
      "metadata": {
        "id": "WQPfsPuf8v10"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AY1yHIUMxO37",
        "outputId": "58657675-0cb6-449c-e023-b83c552a775e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "val_sequences = tokenizer.texts_to_sequences(X_val)\n",
        "test_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Pad sequences to the same length\n",
        "max_len = 100\n",
        "\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_len, padding='post', truncating='post')\n",
        "val_padded = pad_sequences(val_sequences, maxlen=max_len, padding='post', truncating='post')\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_len, padding='post', truncating='post')"
      ],
      "metadata": {
        "id": "_ziDd4MKRcbe"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4Eyv7_txKLK",
        "outputId": "fa17a95b-6e02-4f63-d4c6-69477dad3bbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "547/547 [==============================] - 8s 11ms/step - loss: 0.5513 - accuracy: 0.7238 - val_loss: 0.4632 - val_accuracy: 0.7843\n",
            "Epoch 2/10\n",
            "547/547 [==============================] - 6s 10ms/step - loss: 0.4134 - accuracy: 0.8119 - val_loss: 0.4602 - val_accuracy: 0.7840\n",
            "Epoch 3/10\n",
            "547/547 [==============================] - 7s 12ms/step - loss: 0.3430 - accuracy: 0.8465 - val_loss: 0.5021 - val_accuracy: 0.7776\n",
            "Epoch 4/10\n",
            "547/547 [==============================] - 5s 10ms/step - loss: 0.2944 - accuracy: 0.8718 - val_loss: 0.5160 - val_accuracy: 0.7739\n",
            "Epoch 5/10\n",
            "547/547 [==============================] - 7s 14ms/step - loss: 0.2529 - accuracy: 0.8867 - val_loss: 0.6091 - val_accuracy: 0.7784\n",
            "Epoch 6/10\n",
            "547/547 [==============================] - 6s 11ms/step - loss: 0.2215 - accuracy: 0.8989 - val_loss: 0.6300 - val_accuracy: 0.7747\n",
            "Epoch 7/10\n",
            "547/547 [==============================] - 7s 12ms/step - loss: 0.1991 - accuracy: 0.9053 - val_loss: 0.6415 - val_accuracy: 0.7720\n",
            "Epoch 8/10\n",
            "547/547 [==============================] - 6s 11ms/step - loss: 0.1871 - accuracy: 0.9126 - val_loss: 0.6591 - val_accuracy: 0.7629\n",
            "Epoch 9/10\n",
            "547/547 [==============================] - 10s 18ms/step - loss: 0.1771 - accuracy: 0.9170 - val_loss: 0.7037 - val_accuracy: 0.7688\n",
            "Epoch 10/10\n",
            "547/547 [==============================] - 10s 18ms/step - loss: 0.1569 - accuracy: 0.9250 - val_loss: 0.7380 - val_accuracy: 0.7685\n",
            "118/118 - 0s - loss: 0.6945 - accuracy: 0.7776 - 408ms/epoch - 3ms/step\n",
            "Test Accuracy:  0.7775999903678894\n",
            "118/118 [==============================] - 1s 6ms/step\n",
            "tf.Tensor(\n",
            "[[1878    0]\n",
            " [1872    0]], shape=(2, 2), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Convert comments to sequences of integers\n",
        "\n",
        "\n",
        "# Define model architecture\n",
        "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_size))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if word in word_embeddings.wv:\n",
        "        embedding_matrix[i] = word_embeddings.wv[word]\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(len(tokenizer.word_index) + 1, embedding_size, input_length=max_len,\n",
        "                              weights=[embedding_matrix], trainable=False),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "num_epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "history = model.fit(train_padded, y_train, epochs=num_epochs, batch_size=batch_size, validation_data=(val_padded, y_val))\n",
        "#model.fit(X_train, y_train,validation_data=(X_val, y_val), epochs=10, batch_size=32)\n",
        "# Evaluate model on test set\n",
        "test_loss, test_acc = model.evaluate(test_padded, y_test, verbose=2)\n",
        "print('Test Accuracy: ', test_acc)\n",
        "\n",
        "# Plot confusion matrix\n",
        "y_pred = model.predict(test_padded)\n",
        "y_pred=np.round(y_pred,axis=1)\n",
        "cm = tf.math.confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(test_padded)\n",
        "# y_pred=np.round(y_pred,axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NH5cKcvF_FJj",
        "outputId": "4b4a01d9-8b92-4d09-bb5a-0c0638845802"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "118/118 [==============================] - 1s 7ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYYQdjFm_aFc",
        "outputId": "32d56b4c-546a-4944-9302-26072d2e240c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 1, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "9OJeXUn7_xtt"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred2 = np.round(y_pred).flatten()"
      ],
      "metadata": {
        "id": "PS8FQFW9_TK8"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm1 = confusion_matrix(y_pred2,y_test)"
      ],
      "metadata": {
        "id": "53toX6IHAuEK"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "G7qF5ArN-b3x"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(cm1, annot=True, cmap='Blues', fmt='g')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "s7zvScFv-i7Z",
        "outputId": "b11c7c08-c610-400f-a25f-76be2c2e8eab"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 36
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/MyDrive/Comments/1_model_including_embedding_layer')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csU1SaY1DjU6",
        "outputId": "9d31d6e1-6b17-4e8c-b880-968fa795dca4"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_padded.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzZAe2ZJDjYC",
        "outputId": "f148cc0d-d3aa-404f-967d-dc64cf566e27"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17500, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l89BY-wQxKLK",
        "outputId": "7f7da0f4-0e8b-4de6-979c-3750eca0f144"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "69/69 [==============================] - 4s 12ms/step - loss: 4416.9780 - accuracy: 0.5025 - val_loss: 1092.5513 - val_accuracy: 0.4960\n",
            "Epoch 2/10\n",
            "69/69 [==============================] - 1s 7ms/step - loss: 1992.9703 - accuracy: 0.5029 - val_loss: 555.2484 - val_accuracy: 0.5045\n",
            "Epoch 3/10\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 960.7247 - accuracy: 0.4998 - val_loss: 289.5102 - val_accuracy: 0.5195\n",
            "Epoch 4/10\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 457.7003 - accuracy: 0.5074 - val_loss: 150.9700 - val_accuracy: 0.5197\n",
            "Epoch 5/10\n",
            "69/69 [==============================] - 0s 4ms/step - loss: 223.7118 - accuracy: 0.5033 - val_loss: 81.9097 - val_accuracy: 0.5091\n",
            "Epoch 6/10\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 120.0521 - accuracy: 0.5057 - val_loss: 47.9157 - val_accuracy: 0.5115\n",
            "Epoch 7/10\n",
            "69/69 [==============================] - 0s 4ms/step - loss: 71.0078 - accuracy: 0.5025 - val_loss: 29.4899 - val_accuracy: 0.4963\n",
            "Epoch 8/10\n",
            "69/69 [==============================] - 0s 4ms/step - loss: 41.6888 - accuracy: 0.5046 - val_loss: 20.0260 - val_accuracy: 0.4963\n",
            "Epoch 9/10\n",
            "69/69 [==============================] - 0s 5ms/step - loss: 27.7168 - accuracy: 0.5083 - val_loss: 15.4076 - val_accuracy: 0.5083\n",
            "Epoch 10/10\n",
            "69/69 [==============================] - 0s 4ms/step - loss: 21.2588 - accuracy: 0.5066 - val_loss: 11.2561 - val_accuracy: 0.5099\n",
            "118/118 - 0s - loss: 9.3482 - accuracy: 0.4981 - 175ms/epoch - 1ms/step\n",
            "Test Accuracy:  0.4981333315372467\n"
          ]
        }
      ],
      "source": [
        "# Define model architecture\n",
        "model2 = tf.keras.Sequential([tf.keras.layers.Dense(64, activation='relu',input_shape=(100,)),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "history2 = model2.fit(train_padded, y_train, epochs=10, batch_size=256, validation_data=(val_padded, y_val))\n",
        "\n",
        "# Evaluate model on test set\n",
        "test_loss, test_acc = model2.evaluate(test_padded, y_test, verbose=2)\n",
        "print('Test Accuracy: ', test_acc)\n",
        "\n",
        "# Plot confusion matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "y_pred21 = model2.predict(test_padded)\n",
        "y_pred22=np.round(y_pred21).flatten()\n",
        "cm22 = confusion_matrix(y_test,y_pred22)\n",
        "sns.heatmap(cm22, annot=True, cmap='Blues', fmt='g')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "jpx7_BoNHgRo",
        "outputId": "4c80b9a9-bed4-45ee-c1b5-a10d6b0dcd64"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "118/118 [==============================] - 1s 4ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cm22"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzqcs7rtR9ph",
        "outputId": "1651bfb3-8546-4fef-d7e2-b72bb56713f7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 702,  706],\n",
              "       [1176, 1166]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZxgTSR4xKLL"
      },
      "source": [
        "## Q2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgYcnw-MxKLL",
        "outputId": "cda046f7-fb8a-47c6-ce48-8aa179866fdc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Train Loss: 0.6908220822793724, Val Loss: 0.6887604432590937\n",
            "Epoch 2/10, Train Loss: 0.6521522859587287, Val Loss: 0.6536708019547544\n",
            "Epoch 3/10, Train Loss: 0.6380945688834155, Val Loss: 0.6561519220723944\n",
            "Epoch 4/10, Train Loss: 0.6009903085710359, Val Loss: 0.6484808194435249\n",
            "Epoch 5/10, Train Loss: 0.6114181706505101, Val Loss: 0.6374850202414949\n",
            "Epoch 6/10, Train Loss: 0.4977085384791785, Val Loss: 0.5646135372630621\n",
            "Epoch 7/10, Train Loss: 0.4162685464333444, Val Loss: 0.5453914919141996\n",
            "Epoch 8/10, Train Loss: 0.3510406223418069, Val Loss: 0.5568402677269305\n",
            "Epoch 9/10, Train Loss: 0.3030809630660245, Val Loss: 0.5714961800534847\n",
            "Epoch 10/10, Train Loss: 0.2513226888203708, Val Loss: 0.6129618332547656\n",
            "Simple LSTM Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.74      0.75      1875\n",
            "           1       0.75      0.77      0.76      1875\n",
            "\n",
            "    accuracy                           0.76      3750\n",
            "   macro avg       0.76      0.76      0.76      3750\n",
            "weighted avg       0.76      0.76      0.76      3750\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import string\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "roll = 2345\n",
        "\n",
        "# Load the data\n",
        "def load_data(root_folder):\n",
        "    texts, labels = [], []\n",
        "    for label, subfolder in enumerate(['neg', 'pos']):\n",
        "        folder_path = os.path.join(root_folder, subfolder)\n",
        "        for file in os.listdir(folder_path):\n",
        "            file_path = os.path.join(folder_path, file)\n",
        "            with open(file_path, encoding='utf-8') as f:\n",
        "                texts.append(f.read())\n",
        "                labels.append(label)\n",
        "    return texts, labels\n",
        "\n",
        "root_folder = 'comments250k\\comments250k'\n",
        "texts, labels = load_data(root_folder)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(texts, labels, test_size=0.3, stratify=labels, random_state=roll)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=roll)\n",
        "\n",
        "# Tokenizer\n",
        "def simple_tokenizer(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    return [token for token in tokens if token not in string.punctuation]\n",
        "\n",
        "# Create TextDataset and DataLoader\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_seq_length):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        tokens = self.tokenizer(text)[:self.max_seq_length]\n",
        "        token_ids = [hash(token) % 10000 for token in tokens]\n",
        "        token_ids += [0] * (self.max_seq_length - len(token_ids))\n",
        "        return np.array(token_ids), self.labels[idx]\n",
        "\n",
        "batch_size = 64\n",
        "max_seq_length = 100\n",
        "\n",
        "train_dataset = TextDataset(X_train, y_train, simple_tokenizer, max_seq_length)\n",
        "val_dataset = TextDataset(X_val, y_val, simple_tokenizer, max_seq_length)\n",
        "test_dataset = TextDataset(X_test, y_test, simple_tokenizer, max_seq_length)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# LSTM Model\n",
        "class SimpleLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size):\n",
        "        super(SimpleLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        _, (h_n, _) = self.lstm(x)\n",
        "        return self.fc(h_n.squeeze(0))\n",
        "# RNN Model\n",
        "class SimpleRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size):\n",
        "        super(SimpleRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        _, h_n = self.rnn(x)\n",
        "        return self.fc(h_n.squeeze(0))\n",
        "\n",
        "# GRU Model\n",
        "class SimpleGRU(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size):\n",
        "        super(SimpleGRU, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        _, h_n = self.gru(x)\n",
        "        return self.fc(h_n.squeeze(0))\n",
        "\n",
        "# Bidirectional 3-layer Stacked LSTM Model\n",
        "class BiStackedLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size, num_layers=3):\n",
        "        super(BiStackedLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_size * 2, output_size)  # Multiply by 2 for bidirectional\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        _, (h_n, _) = self.lstm(x)\n",
        "        h_n = torch.cat([h_n[-2,:,:], h_n[-1,:,:]], dim=1)  # Concatenate the hidden states of both directions\n",
        "        return self.fc(h_n)\n",
        "\n",
        "# Training and evaluation functions\n",
        "def train_model(model, train_loader, val_loader, device, n_epochs=10, lr=0.001):\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            with torch.no_grad():\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch {epoch + 1}/{n_epochs}, Train Loss: {running_loss / len(train_loader)}, Val Loss: {val_loss / len(val_loader)}')\n",
        "\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "    return classification_report(y_true, y_pred)\n",
        "\n",
        "# Train and evaluate the SimpleLSTM model\n",
        "vocab_size = 10000\n",
        "embedding_dim = 100\n",
        "hidden_size = 128\n",
        "output_size = 2\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "simple_lstm_model = SimpleLSTM(vocab_size, embedding_dim, hidden_size, output_size)\n",
        "train_model(simple_lstm_model, train_loader, val_loader, device)\n",
        "classification_report_ = evaluate_model(simple_lstm_model, test_loader, device)\n",
        "print(\"Simple LSTM Classification Report:\\n\", classification_report_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V29cnCSdxKLM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "def split_dataset(data_dir, train_ratio, val_ratio, test_ratio):\n",
        "    for class_name in ['neg', 'pos']:\n",
        "        class_dir = os.path.join(data_dir, class_name)\n",
        "        files = os.listdir(class_dir)\n",
        "        random.shuffle(files)\n",
        "        \n",
        "        train_cnt = int(train_ratio * len(files))\n",
        "        val_cnt = int(val_ratio * len(files))\n",
        "        \n",
        "        train_files = files[:train_cnt]\n",
        "        val_files = files[train_cnt:train_cnt+val_cnt]\n",
        "        test_files = files[train_cnt+val_cnt:]\n",
        "        \n",
        "        for split, file_list in zip(['train', 'val', 'test'], [train_files, val_files, test_files]):\n",
        "            split_dir = os.path.join(data_dir, split, class_name)\n",
        "            os.makedirs(split_dir, exist_ok=True)\n",
        "            \n",
        "            for file in file_list:\n",
        "                src = os.path.join(class_dir, file)\n",
        "                dst = os.path.join(split_dir, file)\n",
        "                shutil.copyfile(src, dst)\n",
        "\n",
        "data_dir = 'comments250k\\comments250k'\n",
        "split_dataset(data_dir, 0.7, 0.15, 0.15)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSokT69HxKLN",
        "outputId": "634e7a41-77d9-4e7d-a3aa-fcb4db33feff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
            "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll\n",
            "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
            "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "class CommentsDataset(Dataset):\n",
        "    def __init__(self, data_dir, split, vocab=None):\n",
        "        self.split = split\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.tokenizer = get_tokenizer('basic_english')\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "        \n",
        "        for class_name in ['neg', 'pos']:\n",
        "            class_dir = os.path.join(data_dir, split, class_name)\n",
        "            files = os.listdir(class_dir)\n",
        "            for file in files[:100]:\n",
        "                with open(os.path.join(class_dir, file), 'r') as f:\n",
        "                    text = f.read()\n",
        "                self.data.append(text)\n",
        "                self.labels.append(class_name)\n",
        "                \n",
        "        self.labels = self.label_encoder.fit_transform(self.labels)\n",
        "        \n",
        "        if vocab is None:\n",
        "            self.vocab = build_vocab_from_iterator(self._tokenize_data(), specials=['<pad>', '<unk>'])\n",
        "        else:\n",
        "            self.vocab = vocab\n",
        "    \n",
        "    def _tokenize_data(self):\n",
        "        for text in self.data:\n",
        "            yield self.tokenizer(text)\n",
        "\n",
        "    def _numericalize(self, text):\n",
        "        return [self.vocab[token] if token in self.vocab else self.vocab['<unk>'] for token in self.tokenizer(text)]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = self._numericalize(self.data[index])\n",
        "        return torch.tensor(text), self.labels[index]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    data, labels = zip(*batch)\n",
        "    data = pad_sequence(data, batch_first=True)\n",
        "    labels = torch.tensor(labels)\n",
        "    return data, labels\n",
        "\n",
        "train_dataset = CommentsDataset(data_dir, 'train')\n",
        "val_dataset = CommentsDataset(data_dir, 'val', vocab=train_dataset.vocab)\n",
        "test_dataset = CommentsDataset(data_dir, 'test', vocab=train_dataset.vocab)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNHvnxxRxKLN"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SimpleRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(SimpleRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x, _ = self.rnn(x)\n",
        "        x = self.fc(x[:, -1, :])\n",
        "        return x\n",
        "vocab_size = len(train_dataset.vocab)\n",
        "embedding_dim = 128\n",
        "hidden_dim = 256\n",
        "output_dim = 2\n",
        "\n",
        "model = SimpleRNN(vocab_size, embedding_dim, hidden_dim, output_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grtIdNBVxKLO",
        "outputId": "7b552c8c-064e-4bec-97a4-fd35c834480c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 313/313 [13:18<00:00,  2.55s/it]\n",
            "100%|██████████| 110/110 [00:30<00:00,  3.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5, Train Loss: 0.7018, Train Acc: 49.66%, Val Loss: 0.6938, Val Acc: 49.99%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 313/313 [16:22<00:00,  3.14s/it]\n",
            "100%|██████████| 110/110 [00:29<00:00,  3.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/5, Train Loss: 0.7010, Train Acc: 50.87%, Val Loss: 0.7004, Val Acc: 50.03%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 313/313 [20:41<00:00,  3.97s/it]\n",
            "100%|██████████| 110/110 [00:29<00:00,  3.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/5, Train Loss: 0.6970, Train Acc: 49.95%, Val Loss: 0.7026, Val Acc: 50.06%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 313/313 [09:48<00:00,  1.88s/it]\n",
            "100%|██████████| 110/110 [00:30<00:00,  3.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/5, Train Loss: 0.6973, Train Acc: 50.34%, Val Loss: 0.6940, Val Acc: 49.94%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 313/313 [15:19<00:00,  2.94s/it]\n",
            "100%|██████████| 110/110 [00:29<00:00,  3.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/5, Train Loss: 0.6995, Train Acc: 50.14%, Val Loss: 0.7031, Val Acc: 50.09%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 109/109 [00:30<00:00,  3.53it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RNN Model               precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.54      0.01      0.02      3449\n",
            "         pos       0.50      0.99      0.67      3480\n",
            "\n",
            "    accuracy                           0.50      6929\n",
            "   macro avg       0.52      0.50      0.34      6929\n",
            "weighted avg       0.52      0.50      0.34      6929\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "num_epochs = 5\n",
        "\n",
        "def train(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for inputs, labels in tqdm(dataloader):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        \n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    train_loss = running_loss / len(dataloader)\n",
        "    train_acc = 100. * correct / total\n",
        "    return train_loss, train_acc\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(dataloader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    val_loss = running_loss / len(dataloader)\n",
        "    val_acc = 100. * correct / total\n",
        "    return val_loss, val_acc\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train(model, train_dataloader, criterion, optimizer, device)\n",
        "    val_loss, val_acc = evaluate(model, val_dataloader, criterion, device)\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def predict(model, dataloader, device):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    labels_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(dataloader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            \n",
        "            predictions.extend(predicted.cpu().numpy())\n",
        "            labels_list.extend(labels.cpu().numpy())\n",
        "\n",
        "    return predictions, labels_list\n",
        "\n",
        "predictions, labels = predict(model, test_dataloader, device)\n",
        "print('RNN Model',classification_report(labels, predictions, target_names=['neg', 'pos']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xhh4VBjxKLO"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SimpleLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(SimpleLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x, _ = self.rnn(x)\n",
        "        x = self.fc(x[:, -1, :])\n",
        "        return x\n",
        "\n",
        "vocab_size = len(train_dataset.vocab)\n",
        "embedding_dim = 128\n",
        "hidden_dim = 256\n",
        "output_dim = 2\n",
        "\n",
        "model = SimpleLSTM(vocab_size, embedding_dim, hidden_dim, output_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ww8xtHyOxKLP",
        "outputId": "f440148d-86b4-470a-875f-4e7bcec3206b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 313/313 [43:36<00:00,  8.36s/it] \n",
            "100%|██████████| 110/110 [01:04<00:00,  1.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5, Train Loss: 0.6953, Train Acc: 49.58%, Val Loss: 0.6932, Val Acc: 50.04%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 313/313 [45:26<00:00,  8.71s/it]\n",
            "100%|██████████| 110/110 [01:04<00:00,  1.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/5, Train Loss: 0.6928, Train Acc: 49.83%, Val Loss: 0.6935, Val Acc: 50.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 313/313 [45:34<00:00,  8.74s/it]\n",
            "100%|██████████| 110/110 [01:02<00:00,  1.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/5, Train Loss: 0.6916, Train Acc: 50.45%, Val Loss: 0.6930, Val Acc: 50.14%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 313/313 [43:42<00:00,  8.38s/it]\n",
            "100%|██████████| 110/110 [01:01<00:00,  1.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/5, Train Loss: 0.6910, Train Acc: 50.01%, Val Loss: 0.6932, Val Acc: 50.27%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 313/313 [43:49<00:00,  8.40s/it]\n",
            "100%|██████████| 110/110 [01:02<00:00,  1.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/5, Train Loss: 0.6892, Train Acc: 50.41%, Val Loss: 0.6934, Val Acc: 50.23%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 109/109 [01:08<00:00,  1.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LSTM Model               precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.74      0.01      0.02      3449\n",
            "         pos       0.50      1.00      0.67      3480\n",
            "\n",
            "    accuracy                           0.51      6929\n",
            "   macro avg       0.62      0.50      0.34      6929\n",
            "weighted avg       0.62      0.51      0.35      6929\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "num_epochs = 5\n",
        "\n",
        "def train(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for inputs, labels in tqdm(dataloader):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        \n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    train_loss = running_loss / len(dataloader)\n",
        "    train_acc = 100. * correct / total\n",
        "    return train_loss, train_acc\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(dataloader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    val_loss = running_loss / len(dataloader)\n",
        "    val_acc = 100. * correct / total\n",
        "    return val_loss, val_acc\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train(model, train_dataloader, criterion, optimizer, device)\n",
        "    val_loss, val_acc = evaluate(model, val_dataloader, criterion, device)\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def predict(model, dataloader, device):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    labels_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(dataloader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            \n",
        "            predictions.extend(predicted.cpu().numpy())\n",
        "            labels_list.extend(labels.cpu().numpy())\n",
        "\n",
        "    return predictions, labels_list\n",
        "\n",
        "predictions, labels = predict(model, test_dataloader, device)\n",
        "print('LSTM Model',classification_report(labels, predictions, target_names=['neg', 'pos']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04V4oL7MxKLP"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SimpleGRU(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(SimpleGRU, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x, _ = self.rnn(x)\n",
        "        x = self.fc(x[:, -1, :])\n",
        "        return x\n",
        "vocab_size = len(train_dataset.vocab)\n",
        "embedding_dim = 128\n",
        "hidden_dim = 256\n",
        "output_dim = 2\n",
        "\n",
        "model = SimpleGRU(vocab_size, embedding_dim, hidden_dim, output_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7haMQTXJxKLP",
        "outputId": "07267422-7753-4fbd-d67e-9ec7fe769160"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 313/313 [44:49<00:00,  8.59s/it]\n",
            "100%|██████████| 110/110 [01:35<00:00,  1.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5, Train Loss: 0.6991, Train Acc: 50.31%, Val Loss: 0.6971, Val Acc: 49.97%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 313/313 [41:11<00:00,  7.89s/it] \n",
            "100%|██████████| 110/110 [01:39<00:00,  1.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/5, Train Loss: 0.6942, Train Acc: 50.12%, Val Loss: 0.6935, Val Acc: 50.21%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 313/313 [21:03<00:00,  4.04s/it]\n",
            "100%|██████████| 110/110 [01:42<00:00,  1.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/5, Train Loss: 0.6582, Train Acc: 59.76%, Val Loss: 0.5123, Val Acc: 77.20%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 313/313 [18:03<00:00,  3.46s/it]\n",
            "100%|██████████| 110/110 [01:38<00:00,  1.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/5, Train Loss: 0.3552, Train Acc: 85.58%, Val Loss: 0.2594, Val Acc: 90.12%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 313/313 [17:56<00:00,  3.44s/it]\n",
            "100%|██████████| 110/110 [01:41<00:00,  1.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/5, Train Loss: 0.1783, Train Acc: 93.53%, Val Loss: 0.2011, Val Acc: 92.74%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 109/109 [01:45<00:00,  1.04it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GRU Model               precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.92      0.95      0.93      3449\n",
            "         pos       0.95      0.92      0.93      3480\n",
            "\n",
            "    accuracy                           0.93      6929\n",
            "   macro avg       0.93      0.93      0.93      6929\n",
            "weighted avg       0.93      0.93      0.93      6929\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "num_epochs = 5\n",
        "\n",
        "def train(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for inputs, labels in tqdm(dataloader):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        \n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    train_loss = running_loss / len(dataloader)\n",
        "    train_acc = 100. * correct / total\n",
        "    return train_loss, train_acc\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(dataloader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    val_loss = running_loss / len(dataloader)\n",
        "    val_acc = 100. * correct / total\n",
        "    return val_loss, val_acc\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train(model, train_dataloader, criterion, optimizer, device)\n",
        "    val_loss, val_acc = evaluate(model, val_dataloader, criterion, device)\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def predict(model, dataloader, device):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    labels_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(dataloader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            \n",
        "            predictions.extend(predicted.cpu().numpy())\n",
        "            labels_list.extend(labels.cpu().numpy())\n",
        "\n",
        "    return predictions, labels_list\n",
        "\n",
        "predictions, labels = predict(model, test_dataloader, device)\n",
        "print('GRU Model',classification_report(labels, predictions, target_names=['neg', 'pos']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5aE5YAbxKLQ"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers=3, bidirectional=True, dropout=0.5):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=bidirectional, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.fc(x[:, -1, :])\n",
        "        return x\n",
        "vocab_size = len(train_dataset.vocab)\n",
        "embedding_dim = 128\n",
        "hidden_dim = 256\n",
        "output_dim = 2\n",
        "num_layers = 3\n",
        "bidirectional = True\n",
        "dropout = 0.5\n",
        "\n",
        "model = BiLSTM(vocab_size, embedding_dim, hidden_dim, output_dim, num_layers=num_layers, bidirectional=bidirectional, dropout=dropout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQMEemVzxKLQ",
        "outputId": "309408b8-8c28-4795-9397-7d1e7aeec205"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [03:19<00:00, 49.89s/it]\n",
            "100%|██████████| 4/4 [00:25<00:00,  6.30s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5, Train Loss: 0.7007, Train Acc: 49.50%, Val Loss: 0.7090, Val Acc: 50.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [03:49<00:00, 57.30s/it]\n",
            "100%|██████████| 4/4 [00:25<00:00,  6.48s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/5, Train Loss: 0.6968, Train Acc: 50.50%, Val Loss: 0.6935, Val Acc: 48.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [04:34<00:00, 68.65s/it] \n",
            "100%|██████████| 4/4 [00:28<00:00,  7.00s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/5, Train Loss: 0.7031, Train Acc: 44.50%, Val Loss: 0.7109, Val Acc: 50.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [04:09<00:00, 62.49s/it] \n",
            "100%|██████████| 4/4 [00:27<00:00,  6.90s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/5, Train Loss: 0.6974, Train Acc: 48.50%, Val Loss: 0.7007, Val Acc: 50.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [04:15<00:00, 63.83s/it]\n",
            "100%|██████████| 4/4 [00:22<00:00,  5.67s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/5, Train Loss: 0.6977, Train Acc: 48.50%, Val Loss: 0.6952, Val Acc: 50.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:19<00:00,  4.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bi-directional LSTM Model               precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.50      1.00      0.67       100\n",
            "         pos       0.00      0.00      0.00       100\n",
            "\n",
            "    accuracy                           0.50       200\n",
            "   macro avg       0.25      0.50      0.33       200\n",
            "weighted avg       0.25      0.50      0.33       200\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "num_epochs = 5\n",
        "\n",
        "def train(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for inputs, labels in tqdm(dataloader):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        \n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    train_loss = running_loss / len(dataloader)\n",
        "    train_acc = 100. * correct / total\n",
        "    return train_loss, train_acc\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(dataloader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    val_loss = running_loss / len(dataloader)\n",
        "    val_acc = 100. * correct / total\n",
        "    return val_loss, val_acc\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train(model, train_dataloader, criterion, optimizer, device)\n",
        "    val_loss, val_acc = evaluate(model, val_dataloader, criterion, device)\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def predict(model, dataloader, device):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    labels_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(dataloader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            \n",
        "            predictions.extend(predicted.cpu().numpy())\n",
        "            labels_list.extend(labels.cpu().numpy())\n",
        "\n",
        "    return predictions, labels_list\n",
        "\n",
        "predictions, labels = predict(model, test_dataloader, device)\n",
        "print('Bi-directional LSTM Model',classification_report(labels, predictions, target_names=['neg', 'pos']))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "translator_env2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}