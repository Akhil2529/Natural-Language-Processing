{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5a1824f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from gsdmm import MovieGroupProcess\n",
    "from nltk.tokenize import word_tokenize\n",
    "import warnings\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2b7b5a23",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (Temp/ipykernel_4696/2669927789.py, line 35)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\akhil\\AppData\\Local\\Temp/ipykernel_4696/2669927789.py\"\u001b[1;36m, line \u001b[1;32m35\u001b[0m\n\u001b[1;33m    return filtered_comments\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# Split comments into sentences\n",
    "sentences = []\n",
    "for comment in data:\n",
    "    doc = comment\n",
    "    for sente in sent_tokenize(doc):\n",
    "        sentences.append(sente)\n",
    "\n",
    " # Lowercase the comments\n",
    "lowercase_comments = [comment.lower() for comment in data]\n",
    "\n",
    "# Do tokenization for the dataset\n",
    "tokens = []\n",
    "for comment in lowercase_comments:\n",
    "    doc = tokenizer.tokenize(comment)\n",
    "    tokens.append([x.lower() for x in doc if x.lower() not in stop_words and len(x)>1])\n",
    "# Remove punctuations\n",
    "tokens_without_punctuations = []\n",
    "for t in tokens:\n",
    "    var = []\n",
    "    for t1 in t:\n",
    "        if t1.isalnum:\n",
    "            var.append(t1)\n",
    "    tokens_without_punctuations.append(var)\n",
    "    \n",
    "text_without_punctuations = [' '.join(x) for x in tokens_without_punctuations]\n",
    "\n",
    "filtered_comments = []\n",
    "for comment in text_without_punctuations:\n",
    "    doc = nlp(comment)\n",
    "    filtered_tokens = [token.text for token in doc if not token.is_stop]\n",
    "    filtered_comments.append(\" \".join(filtered_tokens))\n",
    "\n",
    "# Generate the corpus for word embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "596d5de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwell high cartoon comedy ran time programs school life teachers 35 years teaching profession lead believe bromwell high satire closer reality teachers scramble survive financially insightful students right pathetic teachers pomp pettiness situation remind schools knew students saw episode student repeatedly tried burn school immediately recalled high classic line inspector sack teachers student welcome bromwell high expect adults age think bromwell high far fetched pity isn'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_comments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d4231bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d13875fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "spacy.load(\"en_core_web_sm\")\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "stop_words.add(\"movie\")\n",
    "stop_words.add(\"br\")\n",
    "stop_words.add(\"the\")\n",
    "stop_words.add(\"film\")\n",
    "data_folder = r\"C:\\Users\\akhil\\Desktop\\NLP Assignment-2\\comments1k\"\n",
    "data = []\n",
    "for file_name in os.listdir(data_folder):\n",
    "    with open(os.path.join(data_folder, file_name), \"r\", encoding=\"utf-8\") as f:\n",
    "        doc = f.read()\n",
    "        data.append(doc)\n",
    "\n",
    "def prepossessing(data):\n",
    "\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    # Split comments into sentences\n",
    "    sentences = []\n",
    "    for comment in data:\n",
    "        doc = comment\n",
    "        for sente in sent_tokenize(doc):\n",
    "            sentences.append(sente)\n",
    "\n",
    "     # Lowercase the comments\n",
    "    lowercase_comments = [comment.lower() for comment in data]\n",
    "\n",
    "    # Do tokenization for the dataset\n",
    "    tokens = []\n",
    "    for comment in lowercase_comments:\n",
    "        doc = tokenizer.tokenize(comment)\n",
    "        tokens.append([x.lower() for x in doc if x.lower() not in stop_words and len(x)>1])\n",
    "    # Remove punctuations\n",
    "    tokens_without_punctuations = []\n",
    "    for t in tokens:\n",
    "        var = []\n",
    "        for t1 in t:\n",
    "            if t1.isalnum:\n",
    "                var.append(t1)\n",
    "        tokens_without_punctuations.append(var)\n",
    "\n",
    "    text_without_punctuations = [' '.join(x) for x in tokens_without_punctuations]\n",
    "\n",
    "    filtered_comments = []\n",
    "    for comment in text_without_punctuations:\n",
    "        doc = nlp(comment)\n",
    "        filtered_tokens = [token.text for token in doc if not token.is_stop]\n",
    "        filtered_comments.append(\" \".join(filtered_tokens))\n",
    "\n",
    "    return filtered_comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ea5056d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1= prepossessing(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "28f9ef87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e56a449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/rwalk/gsdmm.git\n",
      "  Cloning https://github.com/rwalk/gsdmm.git to c:\\users\\akhil\\appdata\\local\\temp\\pip-req-build-4jvggm9k\n",
      "  Resolved https://github.com/rwalk/gsdmm.git to commit 4ad1b6b6976743681ee4976b4573463d359214ee\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from gsdmm==0.1) (1.20.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/rwalk/gsdmm.git 'C:\\Users\\akhil\\AppData\\Local\\Temp\\pip-req-build-4jvggm9k'\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/rwalk/gsdmm.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfcf3c7",
   "metadata": {},
   "source": [
    "# Topic Modeling \n",
    "1)LDA MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7a52f1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: ['great', 'time', 'like', 'story', 'good', 'yokai', 'movies', 'miike']\n",
      "Topic 1: ['stewart', 'good', 'jeff', 'like', 'gannon', 'story', 'people', 'great']\n",
      "Topic 2: ['films', 'like', 'good', 'great', 'think', 'life', 'young', 'best']\n",
      "Topic 3: ['good', 'like', 'story', 'movies', 'time', 'great', 'life', 'best']\n",
      "Topic 4: ['like', 'star', 'matthau', 'burns', 'time', 'luke', 'best', 'films']\n",
      "Topic 5: ['good', 'like', 'story', 'davies', 'great', 'people', 'movies', 'watch']\n",
      "Topic 6: ['don', 'life', 'school', 'people', 'way', 'time', 'story', 'high']\n",
      "Topic 7: ['star', 'like', 'time', 'good', 'character', 'story', 'love', 'great']\n",
      "Topic 8: ['like', 'red', 'love', 'story', 'la', 'time', 'giallo', 'times']\n",
      "Topic 9: ['ramones', 'brosnan', 'rock', 'man', 'julian', 'story', 'high', 'david']\n",
      "Topics assigned to document 0_9.txt: [[0.00149277 0.00149273 0.00149275 0.00149288 0.00149276 0.00149284\n",
      "  0.00149295 0.00149273 0.98656471 0.00149287]]\n",
      "Topics assigned to document 1_7.txt: [[5.43568121e-04 5.43544839e-04 5.43542007e-04 5.43601153e-04\n",
      "  5.43580422e-04 5.43556344e-04 5.43550675e-04 5.43569132e-04\n",
      "  5.43547124e-04 9.95107940e-01]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "\n",
    "# Convert the corpus into a matrix of word counts\n",
    "vectorizer_count_wc = CountVectorizer()\n",
    "matrix_document = vectorizer_count_wc.fit_transform(data1)\n",
    "words_count = vectorizer_count_wc.get_feature_names()\n",
    "\n",
    "# Train the LDA topic model with 10 topics\n",
    "num_topics = 10\n",
    "lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=50)\n",
    "lda_model.fit(matrix_document)\n",
    "\n",
    "# Print the top 8 words for each topic\n",
    "for id_of_topic, topic_selected in enumerate(lda_model.components_):\n",
    "    indices_of_top_words = topic_selected.argsort()[-8:][::-1]\n",
    "    highest_words = [words_count[i] for i in indices_of_top_words]\n",
    "    print(f\"Topic {id_of_topic}: {highest_words}\")\n",
    "\n",
    "# Assign topics to specific documents\n",
    "doc_0_9_vector = vectorizer_count_wc.transform([data1[0]])\n",
    "doc_1_7_vector = vectorizer_count_wc.transform([data1[1]])\n",
    "doc_0_9_topic_distribution = lda_model.transform(doc_0_9_vector)\n",
    "doc_1_7_topic_distribution = lda_model.transform(doc_1_7_vector)\n",
    "\n",
    "# Print the topics assigned to the documents\n",
    "print(f\"Topics assigned to document 0_9.txt: {doc_0_9_topic_distribution}\")\n",
    "print(f\"Topics assigned to document 1_7.txt: {doc_1_7_topic_distribution}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d558466c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bromwell high cartoon comedy .',\n",
       " 'run time program school life , \" teacher \" .',\n",
       " '35 year teach profession lead believe bromwell high satire close reality \" teacher \" .',\n",
       " \"scramble survive financially , insightful student right pathetic teacher ' pomp , pettiness situation , remind school know student .\",\n",
       " 'see episode student repeatedly try burn school , immediately recall ......... .......... high .']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f775b2",
   "metadata": {},
   "source": [
    "2)GSDMM Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ac128ee5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In stage 0: transferred 880 clusters with 10 clusters populated\n",
      "In stage 1: transferred 268 clusters with 10 clusters populated\n",
      "In stage 2: transferred 107 clusters with 10 clusters populated\n",
      "In stage 3: transferred 78 clusters with 10 clusters populated\n",
      "In stage 4: transferred 48 clusters with 10 clusters populated\n",
      "In stage 5: transferred 28 clusters with 10 clusters populated\n",
      "In stage 6: transferred 41 clusters with 10 clusters populated\n",
      "In stage 7: transferred 33 clusters with 10 clusters populated\n",
      "In stage 8: transferred 40 clusters with 10 clusters populated\n",
      "In stage 9: transferred 37 clusters with 10 clusters populated\n",
      "In stage 10: transferred 43 clusters with 10 clusters populated\n",
      "In stage 11: transferred 33 clusters with 10 clusters populated\n",
      "In stage 12: transferred 29 clusters with 10 clusters populated\n",
      "In stage 13: transferred 40 clusters with 10 clusters populated\n",
      "In stage 14: transferred 33 clusters with 10 clusters populated\n",
      "Topics assigned to document 0_9.txt: (9, 0.3687312687312689)\n",
      "Topics assigned to document 1_7.txt: (9, 0.3687312687312689)\n",
      "[('time', 33), ('story', 32), ('like', 31), ('love', 31), ('films', 26), ('seen', 22), ('think', 21), ('life', 20), ('years', 20), ('way', 19)]\n",
      "[('like', 35), ('story', 34), ('life', 26), ('great', 26), ('people', 25), ('ned', 25), ('good', 24), ('movies', 22), ('time', 21), ('know', 21)]\n",
      "[('christmas', 51), ('best', 45), ('good', 41), ('movies', 40), ('scott', 34), ('time', 34), ('like', 32), ('great', 31), ('story', 30), ('scrooge', 29)]\n",
      "[('good', 45), ('story', 40), ('chess', 33), ('love', 28), ('great', 24), ('watch', 23), ('character', 19), ('end', 19), ('best', 19), ('like', 17)]\n",
      "[('good', 81), ('great', 57), ('think', 51), ('like', 49), ('time', 34), ('actors', 34), ('brosnan', 31), ('movies', 30), ('story', 29), ('don', 27)]\n",
      "[('like', 47), ('great', 43), ('good', 40), ('movies', 33), ('story', 29), ('love', 26), ('people', 24), ('lot', 24), ('characters', 24), ('star', 21)]\n",
      "[('ramones', 35), ('like', 32), ('high', 29), ('stewart', 27), ('school', 26), ('best', 25), ('good', 24), ('rock', 23), ('love', 18), ('time', 18)]\n",
      "[('davies', 23), ('like', 22), ('story', 19), ('great', 18), ('good', 17), ('silent', 16), ('character', 15), ('characters', 15), ('marion', 14), ('comedy', 13)]\n",
      "[('story', 29), ('great', 24), ('like', 23), ('good', 22), ('burns', 21), ('find', 20), ('time', 19), ('matthau', 19), ('seen', 18), ('movies', 16)]\n",
      "[('like', 435), ('story', 340), ('good', 328), ('time', 296), ('life', 250), ('great', 247), ('films', 218), ('love', 210), ('people', 210), ('best', 206)]\n"
     ]
    }
   ],
   "source": [
    "data_variable = [word_tokenize(doc) for doc in data1]\n",
    "dictionary = gensim.corpora.Dictionary(data_variable)\n",
    "# filter extreme cases out of dictionary\n",
    "dictionary.filter_extremes(no_below=15, no_above=1)\n",
    "\n",
    "# create variable containing length of dictionary/vocab\n",
    "vocab_length = len(dictionary)\n",
    "\n",
    "# create BOW dictionary\n",
    "bow_corpus = [dictionary.doc2bow(var2) for var2 in data_variable]\n",
    "gsdmm = MovieGroupProcess(K=10, alpha=0.1, beta=0.3, n_iters=15)\n",
    "\n",
    "# fit GSDMM model\n",
    "y = gsdmm.fit(data_variable, vocab_length)\n",
    "# Assign topics to specific documents\n",
    "doc_0_9_vector = dictionary.doc2bow([data1[0]])\n",
    "doc_1_7_vector = dictionary.doc2bow([data1[1]])\n",
    "doc_0_9_topic_distribution = gsdmm.choose_best_label(doc_0_9_vector)\n",
    "doc_1_7_topic_distribution = gsdmm.choose_best_label(doc_1_7_vector)\n",
    "\n",
    "# Print the topics assigned to the documents\n",
    "print(f\"Topics assigned to document 0_9.txt: {doc_0_9_topic_distribution}\")\n",
    "print(f\"Topics assigned to document 1_7.txt: {doc_1_7_topic_distribution}\")\n",
    "for cluster in range(10):\n",
    "    print(sorted(gsdmm.cluster_word_distribution[cluster].items(), key=lambda k: k[1], reverse=True)[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8490cf",
   "metadata": {},
   "source": [
    "Installing doccano tool for performing text annotation and sentiment analysis\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c6a5fda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: doccano in c:\\users\\akhil\\anaconda3\\lib\\site-packages (1.8.3)\n",
      "Requirement already satisfied: djangorestframework-xml<3.0.0,>=2.0.0 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from doccano) (2.0.0)\n",
      "Requirement already satisfied: django-health-check<4.0.0,>=3.16.5 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from doccano) (3.17.0)\n",
      "Requirement already satisfied: django-cleanup<7.0.0,>=6.0.0 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from doccano) (6.0.0)\n",
      "Requirement already satisfied: environs<10.0.0,>=9.5.0 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from doccano) (9.5.0)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.0.10 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from doccano) (1.2.0)\n",
      "Requirement already satisfied: dj-rest-auth[with-social]<3.0.0,>=2.2.5 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from doccano) (2.2.8)\n",
      "Requirement already satisfied: djangorestframework<4.0.0,>=3.13.1 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from doccano) (3.14.0)\n",
      "Requirement already satisfied: django-rest-polymorphic<0.2.0,>=0.1.9 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from doccano) (0.1.10)\n",
      "Requirement already satisfied: whitenoise<7.0.0,>=6.0.0 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from doccano) (6.4.0)\n",
      "Requirement already satisfied: SQLAlchemy<2.0.0,>=1.4.31 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from doccano) (1.4.32)\n",
      "Requirement already satisfied: pyexcel-xlsx<0.7.0,>=0.6.0 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from doccano) (0.6.0)\n",
      "Requirement already satisfied: auto-labeling-pipeline<0.2.0,>=0.1.21 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from doccano) (0.1.21)\n",
      "Requirement already satisfied: django-filter<22.0,>=21.1 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from doccano) (21.1)\n",
      "Requirement already satisfied: drf-yasg<2.0.0,>=1.20.0 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from doccano) (1.21.5)\n",
      "Requirement already satisfied: flower<2.0.0,>=1.2.0 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from doccano) (1.2.0)\n",
      "Requirement already satisfied: gunicorn<21.0.0,>=20.1.0 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from doccano) (20.1.0)\n",
      "Requirement already satisfied: waitress<3.0.0,>=2.0.0 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from doccano) (2.1.2)\n",
      "Requirement already satisfied: django-allauth==0.50.0 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from doccano) (0.50.0)\n",
      "Requirement already satisfied: django-drf-filepond<0.5.0,>=0.4.1 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from doccano) (0.4.1)\n",
      "Requirement already satisfied: django-cors-headers<4.0.0,>=3.11.0 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from doccano) (3.14.0)\n",
      "Requirement already satisfied: pandas<2.0.0,>=1.4.2 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from doccano) (1.5.3)\n",
      "Requirement already satisfied: django-celery-results==2.4.0 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from doccano) (2.4.0)\n",
      "Requirement already satisfied: dj-database-url<0.6.0,>=0.5.0 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from doccano) (0.5.0)\n",
      "Requirement already satisfied: pyexcel<0.8.0,>=0.7.0 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from doccano) (0.7.0)\n",
      "Requirement already satisfied: seqeval<2.0.0,>=1.2.2 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from doccano) (1.2.2)\n",
      "Requirement already satisfied: Django<5.0.0,>=4.0.2 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from doccano) (4.1.7)\n",
      "Requirement already satisfied: celery<6.0.0,>=5.2.3 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from doccano) (5.2.7)\n",
      "Requirement already satisfied: chardet<5.0.0,>=4.0.0 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from doccano) (4.0.0)\n",
      "Requirement already satisfied: furl<3.0.0,>=2.1.3 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from doccano) (2.1.3)\n",
      "Requirement already satisfied: django-polymorphic<4.0.0,>=3.1.0 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from doccano) (3.1.0)\n",
      "Requirement already satisfied: django-storages[google]<2.0.0,>=1.13.1 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from doccano) (1.13.2)\n",
      "Requirement already satisfied: requests in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from django-allauth==0.50.0->doccano) (2.26.0)\n",
      "Requirement already satisfied: pyjwt[crypto]>=1.7 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from django-allauth==0.50.0->doccano) (2.1.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.3.0 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from django-allauth==0.50.0->doccano) (1.3.1)\n",
      "Requirement already satisfied: python3-openid>=3.0.8 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from django-allauth==0.50.0->doccano) (3.2.0)\n",
      "Requirement already satisfied: boto3 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from auto-labeling-pipeline<0.2.0,>=0.1.21->doccano) (1.26.98)\n",
      "Requirement already satisfied: pydantic in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from auto-labeling-pipeline<0.2.0,>=0.1.21->doccano) (1.10.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from auto-labeling-pipeline<0.2.0,>=0.1.21->doccano) (2.11.3)\n",
      "Requirement already satisfied: pytz>=2021.3 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from celery<6.0.0,>=5.2.3->doccano) (2021.3)\n",
      "Requirement already satisfied: click-plugins>=1.1.1 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from celery<6.0.0,>=5.2.3->doccano) (1.1.1)\n",
      "Requirement already satisfied: click<9.0,>=8.0.3 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from celery<6.0.0,>=5.2.3->doccano) (8.0.3)\n",
      "Requirement already satisfied: click-didyoumean>=0.0.3 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from celery<6.0.0,>=5.2.3->doccano) (0.3.0)\n",
      "Requirement already satisfied: vine<6.0,>=5.0.0 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from celery<6.0.0,>=5.2.3->doccano) (5.0.0)\n",
      "Requirement already satisfied: kombu<6.0,>=5.2.3 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from celery<6.0.0,>=5.2.3->doccano) (5.2.4)\n",
      "Requirement already satisfied: billiard<4.0,>=3.6.4.0 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from celery<6.0.0,>=5.2.3->doccano) (3.6.4.0)\n",
      "Requirement already satisfied: click-repl>=0.2.0 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from celery<6.0.0,>=5.2.3->doccano) (0.2.0)\n",
      "Requirement already satisfied: asgiref<4,>=3.5.2 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from Django<5.0.0,>=4.0.2->doccano) (3.6.0)\n",
      "Requirement already satisfied: sqlparse>=0.2.2 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from Django<5.0.0,>=4.0.2->doccano) (0.4.3)\n",
      "Requirement already satisfied: tzdata in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from Django<5.0.0,>=4.0.2->doccano) (2023.2)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from django-drf-filepond<0.5.0,>=0.4.1->doccano) (1.0.11)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from django-drf-filepond<0.5.0,>=0.4.1->doccano) (1.16.0)\n",
      "Requirement already satisfied: google-cloud-storage>=1.27.0 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from django-storages[google]<2.0.0,>=1.13.1->doccano) (2.7.0)\n",
      "Requirement already satisfied: defusedxml>=0.6.0 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from djangorestframework-xml<3.0.0,>=2.0.0->doccano) (0.7.1)\n",
      "Requirement already satisfied: coreapi>=2.3.3 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from drf-yasg<2.0.0,>=1.20.0->doccano) (2.3.3)\n",
      "Requirement already satisfied: inflection>=0.3.1 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from drf-yasg<2.0.0,>=1.20.0->doccano) (0.5.1)\n",
      "Requirement already satisfied: uritemplate>=3.0.0 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from drf-yasg<2.0.0,>=1.20.0->doccano) (4.1.1)\n",
      "Requirement already satisfied: coreschema>=0.0.4 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from drf-yasg<2.0.0,>=1.20.0->doccano) (0.0.4)\n",
      "Requirement already satisfied: packaging>=21.0 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from drf-yasg<2.0.0,>=1.20.0->doccano) (21.0)\n",
      "Requirement already satisfied: ruamel.yaml>=0.16.13 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from drf-yasg<2.0.0,>=1.20.0->doccano) (0.17.21)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from environs<10.0.0,>=9.5.0->doccano) (1.0.0)\n",
      "Requirement already satisfied: marshmallow>=3.0.0 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from environs<10.0.0,>=9.5.0->doccano) (3.19.0)\n",
      "Requirement already satisfied: humanize in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from flower<2.0.0,>=1.2.0->doccano) (4.6.0)\n",
      "Requirement already satisfied: prometheus-client>=0.8.0 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from flower<2.0.0,>=1.2.0->doccano) (0.11.0)\n",
      "Requirement already satisfied: tornado<7.0.0,>=5.0.0 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from flower<2.0.0,>=1.2.0->doccano) (6.1)\n",
      "Requirement already satisfied: orderedmultidict>=1.0.1 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from furl<3.0.0,>=2.1.3->doccano) (1.0.1)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: dj-rest-auth 2.2.8 does not provide the extra 'with-social'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: setuptools>=3.0 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from gunicorn<21.0.0,>=20.1.0->doccano) (58.0.4)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from pandas<2.0.0,>=1.4.2->doccano) (1.20.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from pandas<2.0.0,>=1.4.2->doccano) (2.8.2)\n",
      "Requirement already satisfied: lml>=0.0.4 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from pyexcel<0.8.0,>=0.7.0->doccano) (0.1.0)\n",
      "Requirement already satisfied: pyexcel-io>=0.6.2 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from pyexcel<0.8.0,>=0.7.0->doccano) (0.6.6)\n",
      "Requirement already satisfied: texttable>=0.8.2 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from pyexcel<0.8.0,>=0.7.0->doccano) (1.6.7)\n",
      "Requirement already satisfied: openpyxl>=2.6.1 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from pyexcel-xlsx<0.7.0,>=0.6.0->doccano) (3.0.9)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from seqeval<2.0.0,>=1.2.2->doccano) (1.0.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from SQLAlchemy<2.0.0,>=1.4.31->doccano) (1.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from click<9.0,>=8.0.3->celery<6.0.0,>=5.2.3->doccano) (0.4.6)\n",
      "Requirement already satisfied: prompt-toolkit in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from click-repl>=0.2.0->celery<6.0.0,>=5.2.3->doccano) (3.0.20)\n",
      "Requirement already satisfied: itypes in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from coreapi>=2.3.3->drf-yasg<2.0.0,>=1.20.0->doccano) (1.2.0)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from google-cloud-storage>=1.27.0->django-storages[google]<2.0.0,>=1.13.1->doccano) (2.16.3)\n",
      "Requirement already satisfied: google-resumable-media>=2.3.2 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from google-cloud-storage>=1.27.0->django-storages[google]<2.0.0,>=1.13.1->doccano) (2.4.1)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from google-cloud-storage>=1.27.0->django-storages[google]<2.0.0,>=1.13.1->doccano) (2.3.2)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from google-cloud-storage>=1.27.0->django-storages[google]<2.0.0,>=1.13.1->doccano) (2.11.0)\n",
      "Requirement already satisfied: amqp<6.0.0,>=5.0.9 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from kombu<6.0,>=5.2.3->celery<6.0.0,>=5.2.3->doccano) (5.1.1)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from openpyxl>=2.6.1->pyexcel-xlsx<0.7.0,>=0.6.0->doccano) (1.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from packaging>=21.0->drf-yasg<2.0.0,>=1.20.0->doccano) (3.0.4)\n",
      "Requirement already satisfied: cryptography<4.0.0,>=3.3.1 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from pyjwt[crypto]>=1.7->django-allauth==0.50.0->doccano) (3.4.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from requests->django-allauth==0.50.0->doccano) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from requests->django-allauth==0.50.0->doccano) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from requests->django-allauth==0.50.0->doccano) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from requests->django-allauth==0.50.0->doccano) (2.0.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.3.0->django-allauth==0.50.0->doccano) (3.2.2)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.6 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from ruamel.yaml>=0.16.13->drf-yasg<2.0.0,>=1.20.0->doccano) (0.2.6)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval<2.0.0,>=1.2.2->doccano) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval<2.0.0,>=1.2.2->doccano) (1.7.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval<2.0.0,>=1.2.2->doccano) (1.1.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from boto3->auto-labeling-pipeline<0.2.0,>=0.1.21->doccano) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from boto3->auto-labeling-pipeline<0.2.0,>=0.1.21->doccano) (0.6.0)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.98 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from boto3->auto-labeling-pipeline<0.2.0,>=0.1.21->doccano) (1.29.98)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from jinja2->auto-labeling-pipeline<0.2.0,>=0.1.21->doccano) (1.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from pydantic->auto-labeling-pipeline<0.2.0,>=0.1.21->doccano) (4.4.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from cryptography<4.0.0,>=3.3.1->pyjwt[crypto]>=1.7->django-allauth==0.50.0->doccano) (1.14.6)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage>=1.27.0->django-storages[google]<2.0.0,>=1.13.1->doccano) (4.22.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage>=1.27.0->django-storages[google]<2.0.0,>=1.13.1->doccano) (1.59.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage>=1.27.0->django-storages[google]<2.0.0,>=1.13.1->doccano) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage>=1.27.0->django-storages[google]<2.0.0,>=1.13.1->doccano) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage>=1.27.0->django-storages[google]<2.0.0,>=1.13.1->doccano) (4.9)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from google-resumable-media>=2.3.2->google-cloud-storage>=1.27.0->django-storages[google]<2.0.0,>=1.13.1->doccano) (1.5.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from prompt-toolkit->click-repl>=0.2.0->celery<6.0.0,>=5.2.3->doccano) (0.2.5)\n",
      "Requirement already satisfied: pycparser in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography<4.0.0,>=3.3.1->pyjwt[crypto]>=1.7->django-allauth==0.50.0->doccano) (2.20)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage>=1.27.0->django-storages[google]<2.0.0,>=1.13.1->doccano) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install doccano"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed152fef",
   "metadata": {},
   "source": [
    "# Entropy Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c7c19947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Accuracy: 0.828\n",
      "\n",
      "Iteration 2:\n",
      "Accuracy: 0.834\n",
      "\n",
      "Iteration 3:\n",
      "Accuracy: 0.851\n",
      "\n",
      "Iteration 4:\n",
      "Accuracy: 0.864\n",
      "\n",
      "Iteration 5:\n",
      "Accuracy: 0.874\n",
      "\n",
      "Iteration 6:\n",
      "Accuracy: 0.879\n",
      "\n",
      "Iteration 7:\n",
      "Accuracy: 0.881\n",
      "\n",
      "Iteration 8:\n",
      "Accuracy: 0.883\n",
      "\n",
      "Iteration 9:\n",
      "Accuracy: 0.886\n",
      "\n",
      "Iteration 10:\n",
      "Accuracy: 0.894\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Generate a synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\n",
    "# Split the dataset into initial training set and pool set\n",
    "X_train, X_pool, y_train, y_pool = train_test_split(X, y, test_size=0.9, random_state=42)\n",
    "# Initialize the active learning loop\n",
    "iterations = 10\n",
    "batch_size = 10\n",
    "model = LogisticRegression(random_state=42)\n",
    "for i in range(iterations):\n",
    "    print(\"Iteration {}:\".format(i+1))\n",
    "    # Train the model on the current training set\n",
    "    model.fit(X_train, y_train)\n",
    "    # Predict the labels of the unlabeled instances in the pool set\n",
    "    y_pool_pred = model.predict(X_pool)\n",
    "    ### below\n",
    "    y_pool_prob = model.predict_proba(X_pool)\n",
    "    entropy = -np.sum(y_pool_prob * np.log(y_pool_prob), axis=1)\n",
    "    query_idx = np.argsort(entropy)[-batch_size:]\n",
    "    ### above\n",
    "    X_query = X_pool[query_idx]\n",
    "    y_query = y_pool[query_idx]\n",
    "    # Add the labeled instances to the training set and remove them from the pool set\n",
    "    X_train = np.concatenate([X_train, X_query])\n",
    "    y_train = np.concatenate([y_train, y_query])\n",
    "    X_pool = np.delete(X_pool, query_idx, axis=0)\n",
    "    y_pool = np.delete(y_pool, query_idx)\n",
    "    # Compute and print the accuracy of the model on the test set\n",
    "    y_test_pred = model.predict(X_pool)\n",
    "    accuracy = accuracy_score(y_pool, y_test_pred)\n",
    "    print(\"Accuracy: {:.3f}\\n\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f390238",
   "metadata": {},
   "source": [
    "# Least Confident Sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "05ba151c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Accuracy: 0.824\n",
      "\n",
      "Iteration 2:\n",
      "Accuracy: 0.830\n",
      "\n",
      "Iteration 3:\n",
      "Accuracy: 0.828\n",
      "\n",
      "Iteration 4:\n",
      "Accuracy: 0.824\n",
      "\n",
      "Iteration 5:\n",
      "Accuracy: 0.821\n",
      "\n",
      "Iteration 6:\n",
      "Accuracy: 0.819\n",
      "\n",
      "Iteration 7:\n",
      "Accuracy: 0.816\n",
      "\n",
      "Iteration 8:\n",
      "Accuracy: 0.813\n",
      "\n",
      "Iteration 9:\n",
      "Accuracy: 0.811\n",
      "\n",
      "Iteration 10:\n",
      "Accuracy: 0.809\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "#Dataste\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\n",
    "# Splitting the dataset into initial training set and pool set\n",
    "X_train, X_pool, y_train, y_pool = train_test_split(X, y, test_size=0.9, random_state=42)\n",
    "# Initializing the active learning loop\n",
    "iterations = 10\n",
    "batch_size = 10\n",
    "model =LogisticRegression(random_state=42)\n",
    "for i in range(iterations):\n",
    "    print(\"Iteration {}:\".format(i+1))\n",
    "    # Training the model on the current training set\n",
    "    model.fit(X_train, y_train)\n",
    "    # Predicting the labels for the labeled set\n",
    "    y_pool_pred = model.predict(X_pool)\n",
    "    # Predict the probabilities \n",
    "    y_pool_prob = model.predict_proba(X_pool)\n",
    "\n",
    "    # Calculate the least confident scores\n",
    "    least_confident_scores = np.min(y_pool_prob, axis=1)\n",
    "\n",
    "    # lowest least confident scores\n",
    "    query_idx = np.argsort(least_confident_scores)[:batch_size]\n",
    "\n",
    "    X_query = X_pool[query_idx]\n",
    "    y_query = y_pool[query_idx]\n",
    "\n",
    "    # labeled instances to the training set and remove them from the pool set\n",
    "    X_train = np.concatenate([X_train, X_query])\n",
    "    y_train = np.concatenate([y_train, y_query])\n",
    "    X_pool = np.delete(X_pool, query_idx, axis=0)\n",
    "    y_pool = np.delete(y_pool, query_idx)\n",
    "\n",
    "    # Compute and print the accuracy of the model on the test set\n",
    "    y_test_pred = model.predict(X_pool)\n",
    "    accuracy = accuracy_score(y_pool, y_test_pred)\n",
    "    print(\"Accuracy: {:.3f}\\n\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7043af9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
