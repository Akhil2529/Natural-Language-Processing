{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "df9309e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: ['br', 'movie', 'film', 'like', 'story', 'great', 'good', 'life']\n",
      "Topic 1: ['stewart', 'jeff', 'gannon', 'good', 'mann', 'like', 'dawson', 'movie']\n",
      "Topic 2: ['br', 'film', 'series', 'good', 'like', 'just', 'story', 'star']\n",
      "Topic 3: ['br', 'movie', 'film', 'good', 'like', 'just', 'great', 'movies']\n",
      "Topic 4: ['movie', 'film', 'game', 'br', 'films', 'life', 'black', 'niven']\n",
      "Topic 5: ['br', 'film', 'love', 'time', 'story', 'way', 'work', 'like']\n",
      "Topic 6: ['br', 'film', 'movie', 'like', 'story', 'good', 'brosnan', 'just']\n",
      "Topic 7: ['br', 'film', 'davies', 'christmas', 'scrooge', 'scott', 'story', 'people']\n",
      "Topic 8: ['br', 'film', 'like', 'spielberg', 'david', 'best', 'movie', 'goldberg']\n",
      "Topic 9: ['br', 'film', 'chess', 'luzhin', 'movie', 'world', 'watson', 'turturro']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mgp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_61636/2031741253.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;31m# Print out the top 8 words for each topic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmgp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster_word_distribution\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m     \u001b[0mtop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtopic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Topic {}: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mgp' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gensim\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from gsdmm import MovieGroupProcess\n",
    "\n",
    "# Load data\n",
    "data_folder = r\"C:\\Users\\akhil\\Desktop\\NLP Assignment-2\\comments1k\"\n",
    "docs = []\n",
    "for file_name in os.listdir(data_folder):\n",
    "    with open(os.path.join(data_folder, file_name), \"r\", encoding=\"utf-8\") as f:\n",
    "        doc = f.read()\n",
    "        docs.append(doc)\n",
    "\n",
    "# Create a CountVectorizer object and transform the documents\n",
    "vectorizer = CountVectorizer(stop_words='english', lowercase=True)\n",
    "X = vectorizer.fit_transform(docs)\n",
    "\n",
    "# LDA model\n",
    "lda = LatentDirichletAllocation(n_components=10, random_state=123)\n",
    "lda.fit(X)\n",
    "\n",
    "# Print out the top 8 words for each topic\n",
    "for i, topic in enumerate(lda.components_):\n",
    "    top_words = [vectorizer.get_feature_names()[j] for j in topic.argsort()[:-9:-1]]\n",
    "    print(\"Topic {}: {}\".format(i, top_words))\n",
    "\n",
    "# # Visualize the LDA model\n",
    "# lda_vis = pyLDAvis.gensim.prepare(lda, X, vectorizer)\n",
    "# pyLDAvis.display(lda_vis)\n",
    "\n",
    "# # GSDMM model\n",
    "# mgp = MovieGroupProcess(K=10, alpha=0.1, beta=0.1, n_iters=30)\n",
    "# y = mgp.fit(docs)\n",
    "\n",
    "# Print out the top 8 words for each topic\n",
    "for i, topic in enumerate(mgp.cluster_word_distribution):\n",
    "    top_words = [vectorizer.get_feature_names()[j] for j in topic.argsort()[:-9:-1]]\n",
    "    print(\"Topic {}: {}\".format(i, top_words))\n",
    "\n",
    "# # Visualize the GSDMM model\n",
    "# doc_topic, topic_term, doc_lengths, term_frequency, vocab = mgp.get_topics()\n",
    "# topic_term_dists = mgp.cluster_word_distribution\n",
    "# gsdmm_vis = pyLDAvis.prepare(topic_term_dists, doc_topic, doc_lengths, vocab, term_frequency)\n",
    "# pyLDAvis.display(gsdmm_vis)\n",
    "\n",
    "# Biterm model\n",
    "biterms = []\n",
    "for doc in docs:\n",
    "    words = doc.split()\n",
    "    if len(words) > 1:\n",
    "        biterms.extend(gensim.models.btm_corpus.Biterms(words, min_count=1).bitmerize(window=2))\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(docs)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "btm = gensim.models.btm_model.BtmModel(biterms, dictionary, num_topics=10, passes=50)\n",
    "print(btm.print_topics())\n",
    "\n",
    "# # Visualize the Biterm model\n",
    "# vis = gensim.models.btmvis.BtmVis(btm, corpus, dictionary)\n",
    "# vis.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3645ba31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/rwalk/gsdmm.git\n",
      "  Cloning https://github.com/rwalk/gsdmm.git to c:\\users\\akhil\\appdata\\local\\temp\\pip-req-build-tmqr6c0g\n",
      "  Resolved https://github.com/rwalk/gsdmm.git to commit 4ad1b6b6976743681ee4976b4573463d359214ee\n",
      "Requirement already satisfied: numpy in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from gsdmm==0.1) (1.20.3)\n",
      "Building wheels for collected packages: gsdmm\n",
      "  Building wheel for gsdmm (setup.py): started\n",
      "  Building wheel for gsdmm (setup.py): finished with status 'done'\n",
      "  Created wheel for gsdmm: filename=gsdmm-0.1-py3-none-any.whl size=4631 sha256=c2a626272447408d257b708f0d7ca4e11e2a8bddb93ec6de1ac66afd97154dd9\n",
      "  Stored in directory: C:\\Users\\akhil\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-u0w7kjs_\\wheels\\81\\2c\\23\\3ff788bcc6063bf30116ad1a06e75d3ba9aad3f7bc4aba765b\n",
      "Successfully built gsdmm\n",
      "Installing collected packages: gsdmm\n",
      "Successfully installed gsdmm-0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone -q https://github.com/rwalk/gsdmm.git 'C:\\Users\\akhil\\AppData\\Local\\Temp\\pip-req-build-tmqr6c0g'\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/rwalk/gsdmm.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9d29c854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: ['miike', 'yokai', 'children', 'takashi', 'war', 'kids', 'hero', 'spirits']\n",
      "Topic 1: ['the', 'and', 'of', 'to', 'it', 'is', 'this', 'in']\n",
      "Topic 2: ['alvin', 'morse', 'david', 'foxx', 'jamie', 'sanders', 'santa', 'bristol']\n",
      "Topic 3: ['the', 'and', 'of', 'is', 'br', 'in', 'to', 'it']\n",
      "Topic 4: ['brosnan', 'kinnear', 'pierce', 'greg', 'his', 'bond', 'davis', 'humor']\n",
      "Topic 5: ['the', 'to', 'and', 'he', 'of', 'is', 'in', 'his']\n",
      "Topic 6: ['the', 'to', 'and', 'of', 'br', 'in', 'is', 'that']\n",
      "Topic 7: ['henry', 'betty', 'boop', 'fleischer', 'little', 'store', 'comic', 'short']\n",
      "Topic 8: ['price', 'vincent', 'sammo', 'magician', 'wax', 'mad', 'house', 'rinaldi']\n",
      "Topic 9: ['jodie', 'hong', 'kong', 'fantasy', 'zu', 'summer', 'average', '1983']\n",
      "Topics assigned to document 0_9.txt: [7.69251963e-04 7.69452311e-04 7.69241679e-04 7.69435072e-04\n",
      " 7.69240231e-04 7.69336834e-04 9.93076228e-01 7.69268797e-04\n",
      " 7.69314281e-04 7.69230982e-04]\n",
      "Topics assigned to document 1_7.txt: [3.05826439e-04 3.05910412e-04 3.05810454e-04 9.97247331e-01\n",
      " 3.05883683e-04 3.05863916e-04 3.05876204e-04 3.05853642e-04\n",
      " 3.05827384e-04 3.05816803e-04]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "data_folder = r\"C:\\Users\\akhil\\Desktop\\NLP Assignment-2\\comments1k\"\n",
    "data = []\n",
    "for file_name in os.listdir(data_folder):\n",
    "    with open(os.path.join(data_folder, file_name), \"r\", encoding=\"utf-8\") as f:\n",
    "        doc = f.read()\n",
    "        data.append(doc)\n",
    "\n",
    "\n",
    "# Convert the corpus into a matrix of word counts\n",
    "count_vectorizer = CountVectorizer()\n",
    "doc_term_matrix = count_vectorizer.fit_transform(data)\n",
    "words = count_vectorizer.get_feature_names()\n",
    "\n",
    "# Train the LDA topic model with 10 topics\n",
    "num_topics = 10\n",
    "lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda_model.fit(doc_term_matrix)\n",
    "\n",
    "# Print the top 8 words for each topic\n",
    "for topic_id, topic in enumerate(lda_model.components_):\n",
    "    top_words_indices = topic.argsort()[-8:][::-1]\n",
    "    top_words = [words[i] for i in top_words_indices]\n",
    "    print(f\"Topic {topic_id}: {top_words}\")\n",
    "\n",
    "# Assign topics to specific documents\n",
    "doc_0_9_vector = count_vectorizer.transform([data[0]])\n",
    "doc_1_7_vector = count_vectorizer.transform([data[1]])\n",
    "doc_0_9_topic_distribution = lda_model.transform(doc_0_9_vector)[0]\n",
    "doc_1_7_topic_distribution = lda_model.transform(doc_1_7_vector)[0]\n",
    "\n",
    "# Print the topics assigned to the documents\n",
    "print(f\"Topics assigned to document 0_9.txt: {doc_0_9_topic_distribution}\")\n",
    "print(f\"Topics assigned to document 1_7.txt: {doc_1_7_topic_distribution}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "394d1a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "48cec370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9a460747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 9, 0, 8, 7, 5, 6, 4, 1, 3], dtype=int64)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(doc_1_7_topic_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd84bbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
